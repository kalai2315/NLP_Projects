{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC2Mr3tLjaPmGG3WSJwzjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalai2315/NLP_Projects/blob/main/nlp_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP components**"
      ],
      "metadata": {
        "id": "-JiR3tl0VIqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "yWfo9xx_VEpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "yxF0ttz3ZsKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download NLTK Resources:\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') #punkt is a tokenizer model used by NLTK to split text into words or sentences.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOQpJAzNaXoQ",
        "outputId": "46d9ea72-2b08-4efc-a5de-d6be6a5a2cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"This is a simple example to show how to remove stopwords in Python.\""
      ],
      "metadata": {
        "id": "OX4dBAC5auBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y82qk3F7axmd",
        "outputId": "82b15bf9-a6e3-4d7e-8e45-469340bc8de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'simple', 'example', 'to', 'show', 'how', 'to', 'remove', 'stopwords', 'in', 'Python', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stop words for English\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr3F7hUea7h5",
        "outputId": "0a2bb6cb-b942-4339-f493-7185f044d1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
        "filtered_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RTfZZp1buCp",
        "outputId": "b0032dca-4b48-4d46-c68d-a6d9a430ed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['simple', 'example', 'show', 'remove', 'stopwords', 'Python', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89QmX-XQccn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, **stemming** and **lemmatization** are two common text preprocessing techniques used in natural language processing (NLP). They help reduce words to their root forms, which can improve the performance of NLP models."
      ],
      "metadata": {
        "id": "ThxWfTrESZsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming** involves reducing words to their base or root form. The nltk library provides a simple way to perform stemming."
      ],
      "metadata": {
        "id": "XN9m5It5S9sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PorterStemmer:**\n",
        "\n",
        "PorterStemmer is a specific stemming algorithm provided by NLTK, which reduces words to their root form."
      ],
      "metadata": {
        "id": "UzLoPvYySDW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVpK38mtmtg5",
        "outputId": "d57f4ebe-b5dd-40b5-970b-565bf70c4b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'easily', 'fairly', 'better', 'best']\n",
            "Stemmed words: ['run', 'run', 'easili', 'fairli', 'better', 'best']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download the NLTK data files (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create a Porter Stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"easily\", \"fairly\", \"better\", \"best\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SnowballStemmer**"
      ],
      "metadata": {
        "id": "OWiGWak7SuIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Download NLTK data files (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create Snowball Stemmer object for English\n",
        "nltk_stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"easily\", \"fairly\", \"better\", \"best\"]\n",
        "\n",
        "# Apply stemming using nltk\n",
        "nltk_stemmed_words = [nltk_stemmer.stem(word) for word in words]\n",
        "\n",
        "# Display results\n",
        "print(f\"{'Original':<15} {'NLTK Stemmed':<15}\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "for original, nltk_stemmed in zip(words, nltk_stemmed_words):\n",
        "    print(f\"{original:<15} {nltk_stemmed:<15}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099BF_mgnCZO",
        "outputId": "3702918d-5f9b-4505-f3d0-8f8f2306fa74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original        NLTK Stemmed   \n",
            "==============================\n",
            "running         run            \n",
            "runs            run            \n",
            "easily          easili         \n",
            "fairly          fair           \n",
            "better          better         \n",
            "best            best           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LancasterStemmer**"
      ],
      "metadata": {
        "id": "PnmaeRWTSzkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "# Download NLTK data files (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create Lancaster Stemmer object\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"easily\", \"fairly\", \"better\", \"best\"]\n",
        "\n",
        "# Apply stemming using Lancaster Stemmer\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(f\"{'Original':<15} {'Lancaster Stemmed':<15}\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "    print(f\"{original:<15} {stemmed:<15}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EuOI082RjEe",
        "outputId": "dc66eb64-b61a-4345-cb7b-cd97566ba2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original        Lancaster Stemmed\n",
            "==============================\n",
            "running         run            \n",
            "runs            run            \n",
            "easily          easy           \n",
            "fairly          fair           \n",
            "better          bet            \n",
            "best            best           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "7nwApZxMTBMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization** reduces words to their base or dictionary form. It is more sophisticated than stemming and often produces more meaningful results. The nltk library also supports lemmatization with the WordNet Lemmatizer."
      ],
      "metadata": {
        "id": "74mKmNbMTIJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "wordnet: This dataset is necessary for the lemmatizer to access the WordNet lexical database, which provides word relationships and base forms.\n",
        "\n",
        "omw-1.4: This is the Open Multilingual WordNet dataset, which supports lemmatization in different languages. It’s required for more comprehensive lemmatization."
      ],
      "metadata": {
        "id": "i1vAe3AMX-Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet Lemmatizer"
      ],
      "metadata": {
        "id": "FnHdcqbiT19L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the NLTK data files (only needed once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Create a WordNet Lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"easily\", \"fairly\", \"better\", \"best\"]\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'pos' specifies part of speech\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3OVtAFvT06c",
        "outputId": "3c7f5d88-a118-4aae-e995-ffe709fea40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'easily', 'fairly', 'better', 'best']\n",
            "Lemmatized words: ['run', 'run', 'easily', 'fairly', 'better', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spacy**: If you prefer using spacy, another popular NLP library, it also supports lemmatization and stemming."
      ],
      "metadata": {
        "id": "MmQrhIWZTdzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English tokenizer, tagger, parser, NER, and dependency parser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example text\n",
        "text = \"running runs easily fairly better best\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract lemmatized tokens\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "\n",
        "# Display results\n",
        "print(f\"{'Original':<15} {'Lemmatized':<15}\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "for original, lemmatized in zip(text.split(), lemmatized_words):\n",
        "    print(f\"{original:<15} {lemmatized:<15}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqv9f5h4nEbY",
        "outputId": "b293b48b-4db7-47b4-bf51-08e576f28560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original        Lemmatized     \n",
            "==============================\n",
            "running         run            \n",
            "runs            run            \n",
            "easily          easily         \n",
            "fairly          fairly         \n",
            "better          well           \n",
            "best            good           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word, TextBlob\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"easily\", \"fairly\", \"better\", \"best\"]\n",
        "\n",
        "# Lemmatize each word using TextBlob\n",
        "lemmatized_words = [Word(word).lemmatize() for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTURnPYmUd4m",
        "outputId": "762136c7-546a-4aae-acb7-6d760f6cc7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'easily', 'fairly', 'better', 'best']\n",
            "Lemmatized words: ['running', 'run', 'easily', 'fairly', 'better', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Normalization:**\n",
        "\n",
        "Text normalization is an essential preprocessing step in natural language processing (NLP) and text analysis. It involves converting text into a standardized format to make it easier to process and analyze. The goal is to reduce variations in the text and ensure consistency."
      ],
      "metadata": {
        "id": "ssn7acGVQqiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize NLP tools\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "normalized_text = normalize_text(text)\n",
        "print(normalized_text)  # Output: \"quick brown fox jump lazy dog\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1I0ED-QP86K",
        "outputId": "c3ea3f7e-b330-4d45-a75e-c2d6ec178e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quick brown fox jump lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bag of words BOW**"
      ],
      "metadata": {
        "id": "nyYTu12g0dCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "v = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the sample text\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "\n",
        "# Get the vocabulary generated by the vectorizer\n",
        "vocabulary = v.vocabulary_\n",
        "\n",
        "# Display the vocabulary\n",
        "print(vocabulary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK8izAk_0a6i",
        "outputId": "83925d5b-9f39-414b-aced-abd9b2542efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-grams**"
      ],
      "metadata": {
        "id": "fS3MVZUNUiYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = CountVectorizer(  =(1,2))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWuVkTJv0jC3",
        "outputId": "3ce9ac98-cb1c-4290-bb84-4684646ada89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thor': 9,\n",
              " 'hathodawala': 2,\n",
              " 'is': 4,\n",
              " 'looking': 7,\n",
              " 'for': 0,\n",
              " 'job': 6,\n",
              " 'thor hathodawala': 10,\n",
              " 'hathodawala is': 3,\n",
              " 'is looking': 5,\n",
              " 'looking for': 8,\n",
              " 'for job': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = CountVectorizer(ngram_range=(1,3))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9r1BBoz0nO9",
        "outputId": "abe6fa17-b418-4908-fb10-49591dd094c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thor': 12,\n",
              " 'hathodawala': 2,\n",
              " 'is': 5,\n",
              " 'looking': 9,\n",
              " 'for': 0,\n",
              " 'job': 8,\n",
              " 'thor hathodawala': 13,\n",
              " 'hathodawala is': 3,\n",
              " 'is looking': 6,\n",
              " 'looking for': 10,\n",
              " 'for job': 1,\n",
              " 'thor hathodawala is': 14,\n",
              " 'hathodawala is looking': 4,\n",
              " 'is looking for': 7,\n",
              " 'looking for job': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TfidfVectorizer**"
      ],
      "metadata": {
        "id": "urXTZEpiUa_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog sat on the log.\"\n",
        "]\n",
        "\n",
        "# Create a TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to a dense format and view the result\n",
        "dense = tfidf_matrix.todense()\n",
        "print(dense)\n",
        "\n",
        "# Get feature names (terms)\n",
        "print(vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkzOReqQ0nLc",
        "outputId": "a9a92d2e-a2fe-4472-9fff-90d7c74cc537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.44554752 0.         0.         0.44554752 0.31701073 0.31701073\n",
            "  0.63402146]\n",
            " [0.         0.44554752 0.44554752 0.         0.31701073 0.31701073\n",
            "  0.63402146]]\n",
            "['cat' 'dog' 'log' 'mat' 'on' 'sat' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word2Vec Embedding**"
      ],
      "metadata": {
        "id": "94A7rsvRUUjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w435g190nJA",
        "outputId": "d043f228-2dc7-46c4-94c9-0bace5f1dda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample corpus\n",
        "sentences = [\n",
        "    [\"the\", \"cat\", \"sits\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"dog\", \"barks\"],\n",
        "    [\"cats\", \"and\", \"dogs\", \"are\", \"friends\"],\n",
        "    [\"the\", \"cat\", \"and\", \"the\", \"dog\", \"are\", \"playing\"]\n",
        "]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"word2vec.model\")\n"
      ],
      "metadata": {
        "id": "eoy7E6L50nGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Get the vector for a specific word\n",
        "cat_vector = model.wv[\"cat\"]\n",
        "print(\"Vector for 'cat':\", cat_vector)\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(\"cat\", topn=5)\n",
        "print(\"Words similar to 'cat':\", similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8gtJd7b0nDi",
        "outputId": "6e91b331-1820-4e9a-aeac-af5d697e9599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'cat': [-0.00713986  0.00124088 -0.00717995 -0.00224818  0.00372279  0.00583649\n",
            "  0.00119641  0.00210052 -0.0041096   0.00722774 -0.00631051  0.00464887\n",
            " -0.00822053  0.00203516 -0.00497694 -0.0042481  -0.00311     0.00565629\n",
            "  0.00580238 -0.00497291  0.00077116 -0.00849946  0.00780949  0.00925927\n",
            " -0.00274503  0.00080148  0.00074716  0.00547721 -0.00860547  0.00058599\n",
            "  0.00687331  0.00223275  0.00112166 -0.00932197  0.00848464 -0.00626474\n",
            " -0.00299564  0.00349585 -0.00077315  0.00141302  0.00178579 -0.0068322\n",
            " -0.00972374  0.00904355  0.00619953 -0.00691385  0.0034063   0.00020263\n",
            "  0.00475308 -0.00712433  0.00403038  0.00434652  0.00996038 -0.0044747\n",
            " -0.00139274 -0.00731768 -0.00970131 -0.00908031 -0.00102227 -0.00650779\n",
            "  0.00485144 -0.00616542  0.00252111  0.00074205 -0.0033922  -0.00097998\n",
            "  0.00998134  0.00914575 -0.00446196  0.00908382 -0.00564441  0.00592973\n",
            " -0.00309761  0.00343544  0.0030169   0.00690159 -0.00237646  0.00877759\n",
            "  0.00759191 -0.00954651 -0.00800932 -0.0076404   0.00292179 -0.00279749\n",
            " -0.00692937 -0.00813085  0.00830932  0.0019925  -0.00932916 -0.004796\n",
            "  0.00313691 -0.0047142   0.00528057 -0.00423439  0.00263915 -0.00804909\n",
            "  0.00621423  0.0048216   0.00078686  0.00301233]\n",
            "Words similar to 'cat': [('and', 0.1702033281326294), ('sits', 0.1501343548297882), ('playing', 0.13904547691345215), ('friends', 0.034870728850364685), ('are', 0.004501866642385721)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find 10 closest words in the vector space\n",
        "model.wv.most_similar(\"cat\", topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUgzFy4u0nBX",
        "outputId": "906bf97f-2ea8-49f7-832e-48a960a374b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 0.1702033281326294),\n",
              " ('sits', 0.1501343548297882),\n",
              " ('playing', 0.13904547691345215),\n",
              " ('friends', 0.034870728850364685),\n",
              " ('are', 0.004501866642385721),\n",
              " ('on', -0.005923843942582607),\n",
              " ('the', -0.027924194931983948),\n",
              " ('dogs', -0.028515880927443504),\n",
              " ('dog', -0.0444510318338871),\n",
              " ('cats', -0.06903860718011856)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}